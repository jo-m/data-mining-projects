\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Data Mining: Learning from Large Data Sets - Fall Semester 2015}
\author{member1@student.ethz.ch\\ member2@student.ethz.ch\\ member3@student.ethz.ch\\}
\date{\today}

\begin{document}
\maketitle

\section*{Approximate near-duplicate search using Locality Sensitive Hashing}
We started by closely following the techniques and algorithms described in the lecture and tutorial sessions. As required, Python 2.7 and numpy were used.
Jonathan started out by hashing each shingle with $r * b$ generated minhash functions. He then proceeded to create the signature column for that video by taking the minimum over all shingles.
In the begin, everything was done in a loop and thus very inefficient. In a next step, the code was vectorized where possible using numpy in order to make it more efficient.

Jonathan also went on to create a function which hashes the signature column and returns the bucket for each band (band size $b$). The band and according bucket as well as the video id and the shingles of that video were then emitted.

For the first submission, Marco detected candidate pairs in the reducer, calculated the Jaccard distance and classified them as duplicates if the distance was big enough. This resulted in a score of 0.79.

For submission two, he classified a candidate pair as duplicates, if they were hashed into the same buckets on six or more bands. With that approach, the achieved score was 0.88.

** TODO Dani hier **

\end{document}
